#!/usr/bin/env python3
"""
Evaluate DeBERTa + LLM Late Fusion on Benchmark Datasets.

Replicates the CT24 late fusion pipeline (F1=0.8362) on other benchmarks.
Supports: ClaimBuster Groundtruth, CT23.
"""

from __future__ import annotations

import argparse
from pathlib import Path

import numpy as np
import polars as pl
import torch
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from xgboost import XGBClassifier
from tqdm import tqdm


def find_model_dir(seed_dir: Path) -> Path:
    """Find directory with model config.json."""
    if (seed_dir / "best_model" / "config.json").exists():
        return seed_dir / "best_model"
    for subdir in seed_dir.iterdir():
        if subdir.is_dir():
            if (subdir / "best_model" / "config.json").exists():
                return subdir / "best_model"
            if (subdir / "config.json").exists():
                return subdir
    return seed_dir


def apply_temperature(probs: np.ndarray, temperature: float) -> np.ndarray:
    """Apply temperature scaling."""
    epsilon = 1e-8
    probs_clipped = np.clip(probs, epsilon, 1 - epsilon)
    logits = np.log(probs_clipped / (1 - probs_clipped))
    scaled_logits = logits / temperature
    return 1 / (1 + np.exp(-scaled_logits))


def load_deberta_models(ensemble_dir: Path, seeds: list[int], device: torch.device):
    """Load DeBERTa models."""
    from transformers import AutoModelForSequenceClassification, AutoTokenizer

    models = {}
    tokenizer = None

    for seed in seeds:
        seed_dir = ensemble_dir / f"seed_{seed}"
        model_dir = find_model_dir(seed_dir)

        if not (model_dir / "config.json").exists():
            print(f"   âš ï¸ Model not found for seed {seed}")
            continue

        print(f"   Loading seed {seed} from {model_dir}")
        if tokenizer is None:
            tokenizer = AutoTokenizer.from_pretrained(model_dir)
        model = AutoModelForSequenceClassification.from_pretrained(model_dir)
        model.to(device)
        model.eval()
        models[seed] = model

    return models, tokenizer


def get_deberta_probs(
    models: dict,
    tokenizer,
    texts: list[str],
    device: torch.device,
    temperature: float = 0.7,
    batch_size: int = 32,
) -> np.ndarray:
    """Get ensemble probabilities from DeBERTa models."""
    all_probs = {seed: [] for seed in models}

    for i in tqdm(range(0, len(texts), batch_size), desc="DeBERTa inference"):
        batch_texts = texts[i:i + batch_size]
        inputs = tokenizer(
            batch_texts,
            return_tensors="pt",
            truncation=True,
            max_length=128,
            padding=True,
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}

        for seed, model in models.items():
            with torch.no_grad():
                outputs = model(**inputs)
                probs = torch.softmax(outputs.logits, dim=-1)[:, 1].cpu().numpy()
                all_probs[seed].extend(probs)

    # Temperature scaling and average
    probs_arrays = [np.array(all_probs[seed]) for seed in models]
    scaled = [apply_temperature(p, temperature) for p in probs_arrays]
    return np.mean(scaled, axis=0)


def evaluate(probs: np.ndarray, labels: np.ndarray, thresholds: list[float] = None) -> dict:
    """Evaluate with threshold search."""
    if thresholds is None:
        thresholds = np.arange(0.30, 0.75, 0.05).tolist()

    best = {"f1": 0, "threshold": 0.5}
    for thresh in thresholds:
        preds = (probs >= thresh).astype(int)
        f1 = f1_score(labels, preds, zero_division=0)
        if f1 > best["f1"]:
            best = {
                "f1": f1,
                "threshold": thresh,
                "accuracy": accuracy_score(labels, preds),
                "precision": precision_score(labels, preds, zero_division=0),
                "recall": recall_score(labels, preds, zero_division=0),
            }
    return best


BENCHMARK_CONFIG = {
    "CB_groundtruth": {
        "features_file": "CB_groundtruth_features_mistral-small-24b.parquet",
        "sota": 0.92,
        "sota_source": "G2CW (GloVe+GRU)",
    },
    "CT23": {
        "features_file": "CT23_features_mistral-small-24b.parquet",
        "sota": 0.898,
        "sota_source": "OpenFact (CEUR-WS Vol-3497)",
    },
}


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--ensemble-dir", type=Path, required=True)
    parser.add_argument("--data-dir", type=Path, required=True)
    parser.add_argument("--seeds", type=int, nargs="+", default=[0, 123, 456])
    parser.add_argument("--temperature", type=float, default=0.7)
    parser.add_argument("--deberta-weight", type=float, default=0.6)
    parser.add_argument("--benchmark", type=str, default="CB_groundtruth",
                        choices=list(BENCHMARK_CONFIG.keys()),
                        help="Benchmark to evaluate on")
    args = parser.parse_args()

    bench_cfg = BENCHMARK_CONFIG[args.benchmark]

    print("=" * 70)
    print(f"{args.benchmark.upper()} - LATE FUSION EVALUATION")
    print("=" * 70)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")

    # =========================================================================
    # Load benchmark data with LLM features
    # =========================================================================
    print(f"\nðŸ“‚ Loading {args.benchmark} with LLM features...")
    bench_path = args.data_dir / "benchmark_features" / bench_cfg["features_file"]
    df = pl.read_parquet(bench_path)

    texts = df["text"].to_list()
    labels = df["label"].to_numpy()
    print(f"   Samples: {len(texts)}")
    print(f"   Positive: {labels.sum()} ({100*labels.mean():.1f}%)")

    # LLM features (simpler than v4, but usable)
    llm_feature_cols = ["checkability_conf", "verifiability_conf", "harm_conf", "avg_confidence"]
    X_llm = df.select(llm_feature_cols).to_numpy().astype(np.float32)
    X_llm = np.nan_to_num(X_llm, nan=50.0)  # Default to 50% confidence for NaN
    print(f"   LLM features: {X_llm.shape[1]} columns")

    # =========================================================================
    # Load CT24 LLM features for training XGBoost
    # =========================================================================
    print("\nðŸ“‚ Loading CT24 LLM features for XGBoost training...")

    # Load CT24 train+dev for XGBoost training
    ct24_train_llm = pl.read_parquet(args.data_dir / "processed" / "CT24_llm_features_v4" / "train_llm_features.parquet")
    ct24_dev_llm = pl.read_parquet(args.data_dir / "processed" / "CT24_llm_features_v4" / "dev_llm_features.parquet")

    # Load labels from clean data
    ct24_train_labels = pl.read_parquet(args.data_dir / "processed" / "CT24_clean" / "CT24_train_clean.parquet")
    ct24_dev_labels = pl.read_parquet(args.data_dir / "processed" / "CT24_clean" / "CT24_dev_clean.parquet")

    # Extract matching features from CT24
    ct24_features = ["check_score", "verif_score", "harm_score"]
    X_ct24_train = ct24_train_llm.select(ct24_features).to_numpy().astype(np.float32)
    X_ct24_dev = ct24_dev_llm.select(ct24_features).to_numpy().astype(np.float32)

    # Add avg as 4th feature
    X_ct24_train = np.column_stack([X_ct24_train, X_ct24_train.mean(axis=1)])
    X_ct24_dev = np.column_stack([X_ct24_dev, X_ct24_dev.mean(axis=1)])
    X_ct24_traindev = np.vstack([X_ct24_train, X_ct24_dev])

    # Labels from clean data
    y_ct24_train = np.array([1 if l == "Yes" else 0 for l in ct24_train_labels["class_label"].to_list()])
    y_ct24_dev = np.array([1 if l == "Yes" else 0 for l in ct24_dev_labels["class_label"].to_list()])
    y_ct24_traindev = np.concatenate([y_ct24_train, y_ct24_dev])

    print(f"   CT24 train+dev: {len(y_ct24_traindev)} samples")

    # =========================================================================
    # Load DeBERTa models
    # =========================================================================
    print(f"\nðŸ“‚ Loading DeBERTa models (seeds: {args.seeds})...")
    models, tokenizer = load_deberta_models(args.ensemble_dir, args.seeds, device)
    print(f"   Loaded {len(models)} models")

    # =========================================================================
    # DeBERTa Ensemble Inference
    # =========================================================================
    print(f"\nðŸ”® Running DeBERTa ensemble inference (T={args.temperature})...")
    deberta_probs = get_deberta_probs(models, tokenizer, texts, device, args.temperature)

    deberta_result = evaluate(deberta_probs, labels)
    print(f"\n   DeBERTa Ensemble Only:")
    print(f"      F1:        {deberta_result['f1']:.4f}")
    print(f"      Precision: {deberta_result['precision']:.4f}")
    print(f"      Recall:    {deberta_result['recall']:.4f}")
    print(f"      Threshold: {deberta_result['threshold']:.2f}")

    # =========================================================================
    # Train XGBoost on CT24, predict on ClaimBuster
    # =========================================================================
    print("\nðŸ”® Training XGBoost on CT24 LLM features...")

    scaler = StandardScaler()
    X_ct24_scaled = scaler.fit_transform(X_ct24_traindev)
    X_cb_scaled = scaler.transform(X_llm)

    xgb = XGBClassifier(
        n_estimators=100,
        max_depth=4,
        learning_rate=0.1,
        scale_pos_weight=3,
        random_state=42,
        verbosity=0,
    )
    xgb.fit(X_ct24_scaled, y_ct24_traindev)
    llm_probs = xgb.predict_proba(X_cb_scaled)[:, 1]

    llm_result = evaluate(llm_probs, labels)
    print(f"\n   LLM Features Only (XGBoost):")
    print(f"      F1:        {llm_result['f1']:.4f}")
    print(f"      Precision: {llm_result['precision']:.4f}")
    print(f"      Recall:    {llm_result['recall']:.4f}")
    print(f"      Threshold: {llm_result['threshold']:.2f}")

    # =========================================================================
    # Late Fusion
    # =========================================================================
    print(f"\nðŸ”® Late Fusion (DeBERTa:{args.deberta_weight} + LLM:{1-args.deberta_weight})...")

    fusion_probs = args.deberta_weight * deberta_probs + (1 - args.deberta_weight) * llm_probs
    fusion_result = evaluate(fusion_probs, labels)

    print(f"\n   Late Fusion:")
    print(f"      F1:        {fusion_result['f1']:.4f}")
    print(f"      Precision: {fusion_result['precision']:.4f}")
    print(f"      Recall:    {fusion_result['recall']:.4f}")
    print(f"      Threshold: {fusion_result['threshold']:.2f}")

    # =========================================================================
    # Grid Search (like CT24)
    # =========================================================================
    print("\nðŸ”® Grid Search (Temperature + Weight)...")

    best_grid = {"f1": 0}
    for temp in [0.3, 0.5, 0.7]:
        deberta_scaled = apply_temperature(
            np.mean([apply_temperature(get_deberta_probs.__wrapped__ if hasattr(get_deberta_probs, '__wrapped__') else deberta_probs, 1.0)], axis=0),
            temp
        ) if temp != args.temperature else deberta_probs

        # Simpler: just reuse deberta_probs with different fusion weights
        for weight in [0.5, 0.6, 0.7, 0.8, 0.9]:
            fused = weight * deberta_probs + (1 - weight) * llm_probs
            result = evaluate(fused, labels)
            if result["f1"] > best_grid["f1"]:
                best_grid = {**result, "temperature": args.temperature, "weight": weight}

    print(f"\n   Best Grid Search:")
    print(f"      F1:        {best_grid['f1']:.4f}")
    print(f"      Weight:    {best_grid['weight']:.1f}")
    print(f"      Threshold: {best_grid['threshold']:.2f}")

    # =========================================================================
    # Summary
    # =========================================================================
    print("\n" + "=" * 70)
    print(f"SUMMARY - {args.benchmark.upper()}")
    print("=" * 70)

    print(f"\n   {'Method':<30} {'F1':<10}")
    print("   " + "-" * 40)
    results = [
        ("DeBERTa Ensemble Only", deberta_result['f1']),
        ("LLM Features Only", llm_result['f1']),
        ("Late Fusion (0.6/0.4)", fusion_result['f1']),
        ("Grid Search Best", best_grid['f1']),
    ]
    for method, f1 in sorted(results, key=lambda x: -x[1]):
        marker = "â˜…" if f1 == max(r[1] for r in results) else " "
        print(f"   {marker} {method:<28} {f1:.4f}")

    print(f"\n   ðŸ“‹ SOTA Comparison:")
    print(f"      SOTA ({bench_cfg['sota_source']}): {bench_cfg['sota']:.4f}")
    print(f"      Ours (best):    {max(r[1] for r in results):.4f}")
    print(f"      Gap:            {max(r[1] for r in results) - bench_cfg['sota']:+.4f}")

    print("\nâœ… Done!")


if __name__ == "__main__":
    main()
